{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12: Statistical Models\n",
    "\n",
    "\"Theories are structures of ideas that explain and interpret facts.\" -Stephen Jay Gould\n",
    "\n",
    "Theory and models: to conduct statistical inference, we usually rely on *statistical models*: sets of assumptions plus mathematical relationships between variables, producing a formal representation of some theory. We are essentially trying to explain the process underlying the generation of our data.\n",
    "\n",
    "Regression modeling steps:\n",
    "\n",
    "  1. think through the relevant theory and assumptions\n",
    "  2. specify a model based on theory\n",
    "  3. collect data and clean/prep it\n",
    "  4. estimate model parameters using the data\n",
    "  5. interpret and report the results\n",
    "\n",
    "There is lots more to cover in a course on regression that we must skip for today's quick overview, such as interactions, transforming variables, handling multicollinearity, handling outliers, conducting diagnostics, etc. That's why there are entire courses dedicated to regression analysis.\n",
    "\n",
    "In this notebook, we will focus on the basics of specifying, estimating, and interpreting regression models. The goal is to make you a knowledgeable consumer of studies that use regression, as well as able to tell stories using regression yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and prep the data\n",
    "\n",
    "The first step in regression analysis is doing all the boring work to collect and load your data, clean it, merge/join it, and otherwise get it in suitable condition for analysis. Here, we want tract census data about each listing's local environs. We can get it by merging tract geometries and census data, then spatial-joining listings to those tracts.\n",
    "\n",
    "### 1a. Load/merge tract geometries and census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tract geometries (shapefile)\n",
    "gdf_tracts = gpd.read_file('../../data/tl_2017_06_tract/')\n",
    "gdf_tracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tract census data (csv file)\n",
    "df_tracts = pd.read_csv('../../data/census_tracts_data_ca.csv', dtype={'GEOID10':str, 'state':str, 'county':str})\n",
    "df_tracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, merge tract geometries and census data together\n",
    "gdf_tracts_data = pd.merge(left=gdf_tracts, right=df_tracts, left_on='GEOID', right_on='GEOID10', how='left')\n",
    "gdf_tracts_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Load listings then spatial-join listings to tracts\n",
    "\n",
    "So we attach local (ie, neighborhood) data about each listing's environs. I just copied this code from Module 7's notebook, so if it looks unfamiliar, review it there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load listings, create geometry column, convert to GeoDataFrame\n",
    "df_listings = pd.read_csv('../../data/listings-la_oc_vc.csv')\n",
    "df_listings['geometry'] = gpd.points_from_xy(x=df_listings['longitude'], y=df_listings['latitude'])\n",
    "gdf_listings = gpd.GeoDataFrame(df_listings, crs='epsg:4326')\n",
    "gdf_listings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember: always make sure CRSs match before a spatial join!\n",
    "gdf_tracts_data.crs == gdf_listings.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they don't match, so project one to the other's CRS\n",
    "gdf_listings = gdf_listings.to_crs(gdf_tracts_data.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now spatial join listings to tracts\n",
    "gdf = gpd.sjoin(gdf_listings, gdf_tracts_data, how='inner', op='within')\n",
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all done: what variables do we have now to work with?\n",
    "gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple regression\n",
    "\n",
    "Simple (aka bivariate) regression has just 2 variables: one is used to predict the other.\n",
    "  \n",
    "  - **Response** variable = what you are predicting (synonyms: dependent variable, outcome variable, regressand)\n",
    "  - **Predictor** variable = what you are using to predict (synonyms: independent variable, feature, covariate, regressor)\n",
    "  \n",
    "The response vector  $y$ is the column of observations of your response variable. The design matrix $X$ is the set of columns of observations of your predictor variables (in simple regression, there's only one column here).\n",
    "\n",
    "In this example, I want to predict rent as a function of square footage. Therefore, I **specify** my model as $y = \\beta_0 + \\beta_1 \\times x_1$ where $y$ represents unit's asking rent and $x_1$ represents unit's square footage. $\\beta_0$ (the intercept, aka constant) and $\\beta_1$ (the coefficient on square footage) are the model parameters to be estimated. My chosen confidence level is 95% (and thus my significance level is 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a response and predictor\n",
    "response = 'rent'\n",
    "predictor = 'sqft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter full dataset to retain only these columns and only rows without nulls in these columns\n",
    "data = gdf[[response, predictor]].dropna()\n",
    "print(data.shape)\n",
    "\n",
    "# create design matrix and response vector\n",
    "X = data[predictor]\n",
    "y = data[response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate a linear regression model with OLS, using statsmodels\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I interpret this regression results table?**\n",
    "\n",
    "Remember my model from earlier: $y = \\beta_0 + \\beta_1 \\times x_1$\n",
    "\n",
    "The coefficient is the estimated relationship between my variables (the slope of the line). The *t*-statistic is my coefficient divided by its standard error. The *p* value tells me the probability of seeing a *t* at least this large, assuming the null hypothesis is true (that the coefficient's value actually equals 0). This is a hypothesis test.\n",
    "\n",
    "The 95% confidence interval spans approximately the coefficient Â± 2 standard errors, and has a 95% probability of containing the true value of the coefficient. Refer to last week's lecture for why!\n",
    "\n",
    "Now that I've estimated my model parameters I can plug them in: $y = 935.07 + 1.07 \\times x_1$\n",
    "\n",
    "So if I know that a unit is 800 sqft in size, I can predict its asking rent (using my model) as \\\\$1,788. We can see this visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret my results in plain language, I would say that a 1 sqft increase in house size is associated with a \\\\$1.07 increase in asking rent. The relationship between rent and sqft is significant at my chosen confidence level (95%).\n",
    "\n",
    "So how good is my model? What does the $R^2$ value tell me?\n",
    "\n",
    "To explain more (and predict better), we need more predictors in our model.\n",
    "\n",
    "## 3. Multiple regression\n",
    "\n",
    "OLS regression with multiple predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a response and predictors\n",
    "response = 'rent'\n",
    "predictors = ['bedrooms', 'sqft']\n",
    "\n",
    "# filter full dataset to retain only these columns and only rows without nulls in these columns\n",
    "data = gdf[[response] +  predictors].dropna()\n",
    "\n",
    "# create design matrix and response vector\n",
    "X = data[predictors]\n",
    "y = data[response]\n",
    "\n",
    "# estimate a linear regression model with OLS, using statsmodels\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I interpret this multiple regression results table?**\n",
    "\n",
    "Each coefficient represents the individual predictor's relationship with the response, while holding all the other predictors constant.\n",
    "\n",
    "A 1 bedroom increase is associated with a \\\\$148 increase in asking rent, all else equal, and a 1 square foot increase is associate with a \\\\$0.92 increase in asking rent, all else equal.\n",
    "\n",
    "I can plug my results into my model to make predictions: rent = 804.46 + 148.17(beds) + 0.92(sqft)\n",
    "\n",
    "So if I know a unit is 2 bedrooms and 800 sqft I can predict its asking rent (using my new model) as \\\\$1,837.\n",
    "\n",
    "For another example of interpreting a regression model in practice, see the discussion in [this article](https://www-tandfonline-com.libproxy2.usc.edu/doi/pdf/10.1080/01944363.2020.1819382?needAccess=true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a response and predictors\n",
    "response = 'rent'\n",
    "predictors = ['bedrooms', 'sqft', 'med_home_value', 'mean_commute_time']\n",
    "\n",
    "# filter full dataset to retain only these columns and only rows without nulls in these columns\n",
    "data = gdf[[response] +  predictors].dropna()\n",
    "\n",
    "# create design matrix and response vector\n",
    "X = data[predictors]\n",
    "y = data[response]\n",
    "\n",
    "# estimate a linear regression model with OLS, using statsmodels\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# try different sets of predictors to increase R2 while keeping the total number of predictors relatively low and theoretically sound\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiple regression with dummy variables\n",
    "\n",
    "A dummy variable is a binary (1 or 0) variable that represents mutually exclusive categories.\n",
    "\n",
    "The trick with dummies in regression analysis is that you can't include all categories: you have to leave out 1 (as a reference group). For example, if I want to know the relationship between asking rent and majority white vs non-majority white neighborhood population, I would only include a majority white dummy in my model (so that non-majority white is the reference group that I'm leaving out of my model). Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy variable: 1 if listing is in majority white tract, otherwise 0\n",
    "gdf['majority_white'] = (gdf['pct_white'] > 50).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a response and predictors\n",
    "response = 'rent'\n",
    "predictors = ['bedrooms', 'sqft', 'med_home_value', 'mean_commute_time', 'majority_white']\n",
    "\n",
    "# filter full dataset to retain only these columns and only rows without nulls in these columns\n",
    "data = gdf[[response] +  predictors].dropna()\n",
    "\n",
    "# create design matrix and response vector\n",
    "X = data[predictors]\n",
    "y = data[response]\n",
    "\n",
    "# estimate a linear regression model with OLS, using statsmodels\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I interpret my dummy variable's coefficient?**\n",
    "\n",
    "\"Compared to listings in non-majority white tracts, listings in majority white tracts are associated with asking rents \\\\$36 higher, all else equal.\"\n",
    "\n",
    "The coefficient represents the \"effect\" (be cautious with causal language) on the response variable of this categorical predictor, compared to the reference group.\n",
    "\n",
    "\n",
    "\n",
    "## 5. Explore relationships and transform variables\n",
    "\n",
    "You should normally do this first, but here I want to show you the before-and-after, so I didn't.\n",
    "\n",
    "In this example, I'm interested in predicting rent. So I want to explore the relationships between rent and my predictors, as well as relationships among those predictors themselves. Common ways to do this are with correlation matrices and pair plots.\n",
    "\n",
    "If I see a nonlinear relationship, I can try to linearize it for better modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "variables = ['rent', 'bedrooms', 'sqft', 'med_home_value', 'mean_commute_time']\n",
    "correlations = gdf[variables].corr().round(2)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual correlation matrix via seaborn heatmap\n",
    "# use vmin, vmax, center to set colorbar scale properly\n",
    "ax = sns.heatmap(correlations, vmin=-1, vmax=1, center=0,\n",
    "                 cmap='coolwarm', square=True, linewidths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pairwise relationships with seaborn\n",
    "grid = sns.pairplot(gdf[variables].dropna(), markers='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the natural log of rent\n",
    "gdf['rent_log'] = np.log(gdf['rent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix using our transformed variable\n",
    "variables = ['rent_log', 'bedrooms', 'sqft', 'med_home_value', 'mean_commute_time']\n",
    "correlations = gdf[variables].corr().round(2)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a response and predictors\n",
    "response = 'rent_log'\n",
    "predictors = ['bedrooms', 'sqft', 'med_home_value', 'mean_commute_time']\n",
    "\n",
    "# filter full dataset to retain only these columns and only rows without nulls in these columns\n",
    "data = gdf[[response] +  predictors].dropna()\n",
    "\n",
    "# create design matrix and response vector\n",
    "X = data[predictors]\n",
    "y = data[response]\n",
    "\n",
    "# estimate a linear regression model with OLS, using statsmodels\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do I interpret coefficients when the response is log-transformed?**\n",
    "\n",
    "If the response is log-transformed but the predictor is not (a \"log-linear\" model), then the coefficient is called a \"semi-elasticity\" and (when multiplied by 100) it approximately represents the percent change in the response given a one-unit increase in the predictor.\n",
    "\n",
    "Example: each 1-bedroom increase is associated with a 14% increase in asking rent, and each 1-minute increase in commute time is associated with a 1% decrease in asking rent.\n",
    "\n",
    "If both the response and the predictor are log-transformed (a \"log-log\" model), then the coefficient is called an \"elasticity\" and it approximately represents the percent change in the response given a 1% increase in the predictor. See [this paper](https://doi.org/10.1177/0308518X19869678) for examples/discussion of interpreting log-linear and log-log models in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd534)",
   "language": "python",
   "name": "ppd534"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
