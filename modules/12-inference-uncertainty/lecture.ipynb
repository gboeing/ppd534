{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ez-JmT2mtbz"
   },
   "source": [
    "# Inferential Statistics\n",
    "\n",
    "Statistical inference is the process of using a sample to *infer* the characteristics of an underlying population (from which this sample was drawn) through estimation and hypothesis testing. Contrast this with descriptive statistics, which focus simply on describing the characteristics of the sample itself.\n",
    "\n",
    "Common goals of inferential statistics include:\n",
    "\n",
    "  - parameter estimation and confidence intervals\n",
    "  - hypothesis rejection\n",
    "  - prediction\n",
    "  - model selection\n",
    "\n",
    "Throughout, we will utilize the \"frequentist\" statistical paradigm:\n",
    "    \n",
    "  - frequentists think of probability as proportion of time some outcome occurs (relative frequency)\n",
    "  - given lots of repeated trials, how likely is the observed outcome?\n",
    "  - central concepts: statistical hypothesis testing, *p*-values, confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvvBAk_Cnjq2"
   },
   "outputs": [],
   "source": [
    "!uv pip install numpy pandas seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0G4JI08mtb0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "np.random.seed(0)  # consistent randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9_dcNwAmtb1"
   },
   "outputs": [],
   "source": [
    "# load the CA tracts census data\n",
    "tracts = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/gboeing/ppd534/main/data/census_tracts_data_ca.csv\",\n",
    "    dtype={\"GEOID10\": str},\n",
    ").set_index(\"GEOID10\")\n",
    "tracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGTX1xcLmtb2"
   },
   "outputs": [],
   "source": [
    "tracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NT06D9x-mtb2"
   },
   "source": [
    "## 1. Standardization and *z*-scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aATG24_Lmtb2"
   },
   "outputs": [],
   "source": [
    "# create variable x of the tracts' median age column\n",
    "x = tracts[\"median_age\"].dropna()\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeJeWvrBmtb3"
   },
   "outputs": [],
   "source": [
    "# raw data\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NzEwTDAmtb3"
   },
   "outputs": [],
   "source": [
    "# calculate z-scores (i.e., standardize the median age raw data)\n",
    "x_stdrd = (x - x.mean()) / x.std()\n",
    "x_stdrd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2_RkM6dmtb3"
   },
   "source": [
    "## 2. Central limit theorem and standard errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIPFC9Z7mtb4"
   },
   "outputs": [],
   "source": [
    "# median household income is not normally distributed\n",
    "pop = tracts[\"med_household_income\"].dropna()\n",
    "ax = sns.histplot(pop, stat=\"density\", label=\"observed\", color=\"orange\")\n",
    "ax.set_xlim(-50000, 250000)\n",
    "\n",
    "# simulate random normally-distributed data with the same mean and std\n",
    "normal = np.random.normal(loc=pop.mean(), scale=pop.std(), size=100000)\n",
    "ax = sns.histplot(normal, stat=\"density\", label=\"normal\", zorder=-1)\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZDYO2GLmtb6"
   },
   "outputs": [],
   "source": [
    "# draw 1 simple random sample (of size n) and calculate its mean\n",
    "n = 100  # sample size\n",
    "sample = pop.sample(n)\n",
    "sample.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buRfjzegmtb6"
   },
   "outputs": [],
   "source": [
    "# simulate the CLT: draw 200 random samples and calculate their means\n",
    "num_of_samples = 200\n",
    "sample_means = []\n",
    "for _ in range(num_of_samples):\n",
    "    sample = pop.sample(n)\n",
    "    sample_means.append(sample.mean())\n",
    "\n",
    "# the sample mean is different every time, due to randomness\n",
    "sample_means[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DLv7iMkmtb6"
   },
   "outputs": [],
   "source": [
    "# plot the distribution of all the sample means we calculated\n",
    "ax = sns.histplot(sample_means, stat=\"density\", kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGUqZG5cmtb7"
   },
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# experiment with different numbers of samples and different sample sizes above\n",
    "# what happens to our curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXjTwzuomtb7"
   },
   "source": [
    "The plot above visualizes the (simulated) **sampling distribution** of the mean. The **standard error** of the mean is the standard deviation of its sampling distribution and is formally calculated as the standard deviation of the population divided by the square root of the sample size. The standard error measures how much random variation we expect from samples of a given size drawn from a population, just due to chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnWm1lrBmtb7"
   },
   "outputs": [],
   "source": [
    "# usual calculation of the standard error (i.e., pop's std dev divided by square root of sample size)\n",
    "pop.std() / n**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AExJlxxlmtb7"
   },
   "outputs": [],
   "source": [
    "# our simulated estimate of the standard error (i.e., the std dev of the sampling distribution)\n",
    "pd.Series(sample_means).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lsq8YFiamtb7"
   },
   "source": [
    "The **central limit theorem** tells us that if you have a large enough sample size (typically *n* > 40), the sampling distribution of the mean will be normally distributed even if the variable is not. In our example, median household income values are not normally distributed. But if we take lots of samples (as in our simulation above), we can visualize that their means are indeed normally distributed.\n",
    "\n",
    "This is the fundamental property that allows us to conduct inference with specific levels of confidence, based on our knowledge of the shape of the normal distribution (and the related *t*-distribution, which we use when we don't know the population's distribution). Remember the 68–95–99.7 rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yVTyE-mmtb7"
   },
   "source": [
    "## 3. Estimating parameters and confidence intervals\n",
    "\n",
    "**Confidence intervals** rely on the concept of a **margin of error** which in turn relies on the concept of the **standard error**. We will use statistical inference to estimate a population parameter from a sample. Here, our population = all tracts in California and the parameter we want to estimate is the mean tract median household income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjmPDv34mtb8"
   },
   "outputs": [],
   "source": [
    "# population's average tract-level median income\n",
    "pop = tracts[\"med_household_income\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85JMyolPmtb8"
   },
   "source": [
    "Now let's *pretend* we don't actually know the full population, but can only study a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g79m4Ot4mtb8"
   },
   "outputs": [],
   "source": [
    "# draw a simple random sample\n",
    "n = 100\n",
    "sample = pop.sample(n).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q78SOCyUmtb8"
   },
   "outputs": [],
   "source": [
    "# calculate a descriptive statistic from the sample\n",
    "sample.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvWzDCf3mtb8"
   },
   "source": [
    "How similar is our sample mean to our population mean? Is it a good estimate? We can't evaluate this if we don't know the full population!\n",
    "\n",
    "We have calculated a **point estimate** of the population mean. Let's calculate an **interval estimate** of it instead.\n",
    "\n",
    "Workflow:\n",
    "\n",
    "  1. Determine your desired confidence level.\n",
    "  2. Calculate your sample's degrees of freedom, mean, and standard error of the mean\n",
    "  3. Calculate the interval estimate using the *t*-distribution (because population std dev is \"unknown\")\n",
    "  \n",
    "Recall that earlier we said the standard error = the standard deviation of the population divided by the square root of the sample size. If we don't know the standard deviation of the population, we have to estimate it using the standard deviation of the sample instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBRzdAQ4mtb9"
   },
   "outputs": [],
   "source": [
    "# calculate confidence interval using t-distribution\n",
    "conf = 0.95  # my chosen confidence level\n",
    "mean = sample.mean()  # the sample's mean\n",
    "sem = sample.std() / sample.count() ** 0.5  # estimated standard error of the mean\n",
    "dof = sample.count() - 1  # degrees of freedom\n",
    "lower, upper = stats.t.interval(conf, dof, loc=mean, scale=sem)\n",
    "\n",
    "# with 95% confidence, the true population mean is between these two values\n",
    "round(lower), round(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WqUu_BAmtb9"
   },
   "outputs": [],
   "source": [
    "# calculate the margin of error\n",
    "# our confidence interval = the mean ± the margin of error\n",
    "moe = upper - sample.mean()\n",
    "moe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdBzc1drmtb9"
   },
   "source": [
    "The margin of error represents our statistical **uncertainty**. To look at this another way... the margin of error at 95% confidence is *approximately* 2 standard errors of the statistic, because 95% of the statistic's (normal) sampling distribution is within *approximately* 2 standard deviations of its mean. Similar for 99.7% confidence and 3 approx standard errors. Remember the 68–95–99.7 rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uhys3SHDmtb9"
   },
   "outputs": [],
   "source": [
    "# what is the value of 2 standard errors (of the mean)\n",
    "sem * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1rnlAb4mtb9"
   },
   "outputs": [],
   "source": [
    "# print out the interval estimate nicely (with fancy complicated python string formatting)\n",
    "print(f\"{lower:0.0f} – {upper:0.0f} ({conf * 100:0.0f}% confidence interval)\")\n",
    "print(f\"{mean:0.0f} ± {moe:0.0f} (at {conf * 100:0.0f}% confidence level)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTxoJK76mtb-"
   },
   "source": [
    "We are 95% confident that this interval contains the true population parameter value. That is, if we were to repeat this process many times (sampling then computing CI), on average 95% of the CIs would contain the true population parameter value (and, importantly, 5% wouldn't). One note: remember that you cannot just compare two estimates' MOEs to see if they overlap as proof of insignificance.\n",
    "\n",
    "What happens if your CI includes 0? Remember the ACS and its margins of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxJm2OImmtb-"
   },
   "outputs": [],
   "source": [
    "# so, what is the true population parameter value?\n",
    "pop.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KX3F2dV0mtb-"
   },
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# try different sample sizes and confidence levels: how do these change the interval estimate's size?\n",
    "# then randomly sample 100 tract-level median home values then calculate the mean and 99% confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqfMrAvSmtb-"
   },
   "source": [
    "## 4. Hypothesis testing: *t*-tests and difference-in-means\n",
    "\n",
    "Is the difference between two groups statistically significant?\n",
    "\n",
    "Let's say I'm interested in the home value race/ethnicity gap. Home values are a key pathway to wealth-building in America, but structural gaps exist between different racial/ethnic groups as a function of history, politics, immigration, and discrimination. Given that theory, I want to hypothesis test it. My hypothesis is that typical home values in majority black tracts are higher than those in majority hispanic tracts.\n",
    "\n",
    "For the sake of this example, let's pretend like these tracts are just a sample of all the tracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5-OMAkkmtb-"
   },
   "outputs": [],
   "source": [
    "# choose a variable\n",
    "var = \"med_home_value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM56JekKmtb-"
   },
   "outputs": [],
   "source": [
    "# first get only LA county tracts\n",
    "tracts_la = tracts[tracts[\"county_name\"] == \"Los Angeles\"]\n",
    "\n",
    "# let group1 = our variable of interest in black majority tracts\n",
    "black_tracts = tracts_la[tracts_la[\"pct_black\"] > 50]\n",
    "group1 = black_tracts[var].dropna()\n",
    "\n",
    "# let group2 = our variable of interest in hispanic majority tracts\n",
    "hispanic_tracts = tracts_la[tracts_la[\"pct_hispanic\"] > 50]\n",
    "group2 = hispanic_tracts[var].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vyqfoo7mtb-"
   },
   "outputs": [],
   "source": [
    "# what are the distributions of these two groups?\n",
    "ax = sns.histplot(group1, label=\"group1\", stat=\"density\")\n",
    "ax = sns.histplot(group2, label=\"group2\", stat=\"density\", color=\"orange\", ax=ax)\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jzr9PTFwmtb_"
   },
   "outputs": [],
   "source": [
    "print(int(group1.mean()))\n",
    "print(int(group2.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUeHAlC2mtb_"
   },
   "outputs": [],
   "source": [
    "# calculate difference in means\n",
    "group1.mean() - group2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIo9e9ihmtb_"
   },
   "source": [
    "The average majority-black tract's median home value is approx $20,000 higher than the average majority-hispanic tract's. That seems like a lot! But is it **statistically significant**? To determine this, we calculate the *t*-statistic and its *p*-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq9Ykrnomtb_"
   },
   "outputs": [],
   "source": [
    "# compute the t-stat and its p-value\n",
    "t_statistic, p_value = stats.ttest_ind(group1, group2, equal_var=False, nan_policy=\"omit\")\n",
    "p_value = p_value / 2  # convert two-tailed p-value to one-tailed\n",
    "print(t_statistic)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz7rGKxfmtb_"
   },
   "source": [
    "There is a 12% chance of getting a *t*-statistic at least this large (in a one-tailed *t*-test) just by random chance from sampling error. Notice that I converted my two-tailed p-value to a one-tailed p-value because my hypothesis is directional: it proposes that group 1's mean is greater than group 2's, rather than them just being unequal to each other.\n",
    "\n",
    "A *t*-statistic is similar to a *z*-score in that it tells us how large some statistic is in relation to its standard error, using the *t*-distribution rather than the normal distribution. We use it to tell us how likely it is that we'd see a value of this magnitude just from random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9ybgloKmtb_"
   },
   "outputs": [],
   "source": [
    "# is the difference in means statistically significant?\n",
    "alpha = 0.05  # significance level\n",
    "p_value < alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2b7_avfmtcA"
   },
   "source": [
    "Remember my original hypothesis: \"home values in majority black are higher than those in majority hispanic tracts.\" Let's express it formally in the parlance of statistical hypothesis testing:\n",
    "\n",
    "  - H0: majority-black tract home values are not higher (null hypothesis)\n",
    "  - H1: majority-black tract home values are higher (alternative hypothesis)\n",
    "\n",
    "The two possible outcomes of a hypothesis test are 1) I reject the null hypothesis or 2) I cannot reject the null hypothesis.\n",
    "\n",
    "My *p*-value is not less than the desired significance level, therefore I cannot reject the null hypothesis. You can only reject the null hypothesis if *p* is less than the significance level (which itself is an arbitrarily chosen probability threshold). Rejecting the null hypothesis does not mean that we've proven the alternative hypothesis, but rather just that it provides some evidence for this alternative. Remember Karl Popper and falsification.\n",
    "\n",
    "\n",
    "  - **Type I Error** (false positive): reject the null hypothesis when it is true\n",
    "  - **Type II Error** (false negative): fail to reject the null hypothesis when it is false\n",
    "\n",
    "Our alpha value (significance level) is the Type I error rate that we are willing to accept. Essentially 1 in 20 times we will claim a significant difference just due to random chance. The beta value is the Type II error rate, and 1 - beta = your statistical power (probability of correctly rejecting the null hypothesis when the alternative hypothesis is true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWLyLNMWmtcA"
   },
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# what is the difference in mean tract-level median home values in majority white vs majority black tracts?\n",
    "# is it statistically significant?\n",
    "# what if you randomly sample just 25 tracts from each group: is their difference significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uSeYFw4mtcA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ppd430",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
