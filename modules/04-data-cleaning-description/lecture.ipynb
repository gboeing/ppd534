{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqmo4i4vWRq7"
   },
   "source": [
    "# Data cleaning and descriptive stats\n",
    "\n",
    "Overview of today's learning goals:\n",
    "\n",
    "  1. Introduce pandas\n",
    "  2. Load data files\n",
    "  3. Clean and process data\n",
    "  4. Select, filter, and slice data from a dataset\n",
    "  5. Descriptive stats: central tendency and dispersion\n",
    "  6. Merging and concatenating datasets\n",
    "  7. Grouping and summarizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Us0b4fSqWRq-"
   },
   "outputs": [],
   "source": [
    "!uv pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIC1UJnYWRq-"
   },
   "outputs": [],
   "source": [
    "# something new: import these packages to work with data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "\n",
    "# mount your google drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24f_bpW2WRq-"
   },
   "source": [
    "## 1. Introducing pandas\n",
    "\n",
    "https://pandas.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsoqpnEwWRq-"
   },
   "outputs": [],
   "source": [
    "# review: a python list is a built-in data type\n",
    "my_list = [8, 6, 4, 2]\n",
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jsicDACWRq_"
   },
   "outputs": [],
   "source": [
    "# a numpy array is like a list\n",
    "# but faster, more compact, and lots more features\n",
    "my_array = np.array(my_list)\n",
    "my_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irTv1rAEWRq_"
   },
   "source": [
    "pandas has two primary data structures we will work with: Series and DataFrames\n",
    "\n",
    "### 1a. pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bomsYQAhWRq_"
   },
   "outputs": [],
   "source": [
    "# a pandas series is based on a numpy array: it's fast, compact, and has more functionality\n",
    "# perhaps most notably, it has an index which allows you to work naturally with tabular data\n",
    "my_series = pd.Series(my_list)\n",
    "my_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVSkAqk7WRq_"
   },
   "outputs": [],
   "source": [
    "# look at a list-representation of the index\n",
    "my_series.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DC1CP_OWRq_"
   },
   "outputs": [],
   "source": [
    "# look at the series' values themselves\n",
    "my_series.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-PWUDzaWRq_"
   },
   "outputs": [],
   "source": [
    "# what's the data type of the series' values?\n",
    "type(my_series.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7J6w0Tu1WRq_"
   },
   "outputs": [],
   "source": [
    "# what's the data type of the individual values themselves?\n",
    "my_series.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SABrJkFWRq_"
   },
   "source": [
    "### 1b. pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyKZcb7wWRq_"
   },
   "outputs": [],
   "source": [
    "# a dict can contain multiple lists and label them\n",
    "my_dict = {\n",
    "    \"hh_income\": [75125, 22075, 31950, 115400],\n",
    "    \"home_value\": [525000, 275000, 395000, 985000],\n",
    "}\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPXCongaWRq_"
   },
   "outputs": [],
   "source": [
    "# a pandas dataframe can contain one or more columns\n",
    "# each column is a pandas series\n",
    "# each row is a pandas series\n",
    "# you can create a dataframe by passing in a list, array, series, or dict\n",
    "df = pd.DataFrame(my_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXSVXVo4WRrA"
   },
   "outputs": [],
   "source": [
    "# the row labels in the index are accessed by the .index attribute of the DataFrame object\n",
    "df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NtVG3dkWRrA"
   },
   "outputs": [],
   "source": [
    "# the column labels are accessed by the .columns attribute of the DataFrame object\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuEEQksaWRrA"
   },
   "outputs": [],
   "source": [
    "# the data values are accessed by the .values attribute of the DataFrame object\n",
    "# this is a numpy (two-dimensional) array\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDBhH83TWRrA"
   },
   "source": [
    "## 2. Loading data\n",
    "\n",
    "In practice, you'll work with data by loading a dataset file into pandas. CSV is the most common format. But pandas can also ingest tab-separated data, JSON, and proprietary file formats like Excel .xlsx files, Stata, SAS, and SPSS.\n",
    "\n",
    "Below, notice what pandas's `read_csv` function does:\n",
    "\n",
    "1. recognize the header row and get its variable names\n",
    "1. read all the rows and construct a pandas DataFrame (an assembly of pandas Series rows and columns)\n",
    "1. construct a unique index, beginning with zero\n",
    "1. infer the data type of each variable (ie, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaUGv8wGWRrA"
   },
   "outputs": [],
   "source": [
    "# load a data file\n",
    "# note the dtype argument! always specify that fips codes are strings, otherwise pandas guesses int\n",
    "filepath = \"https://raw.githubusercontent.com/gboeing/ppd534/main/data/census_tracts_data_la.csv\"\n",
    "df = pd.read_csv(filepath, dtype={\"GEOID10\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhF9Y-NqWRrA"
   },
   "outputs": [],
   "source": [
    "# dataframe shape as rows, columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOw1WYVKWRrA"
   },
   "outputs": [],
   "source": [
    "# or use len to just see the number of rows\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLnk0AOYWRrA"
   },
   "outputs": [],
   "source": [
    "# view the dataframe's \"head\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_8J74AWWRrA"
   },
   "outputs": [],
   "source": [
    "# view the dataframe's \"tail\"\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBmrOFlxWRrA"
   },
   "source": [
    "#### What are these data?\n",
    "\n",
    "I gathered them from the census bureau (2017 5-year tract-level ACS) for you, then gave them meaningful variable names. It's a set of socioeconomic variables across all LA County census tracts:\n",
    "\n",
    "|column|description|\n",
    "|------|-----------|\n",
    "|total_pop|Estimate!!SEX AND AGE!!Total population|\n",
    "|median_age|Estimate!!SEX AND AGE!!Total population!!Median age (years)|\n",
    "|pct_hispanic|Percent Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)|\n",
    "|pct_white|Percent Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Not Hispanic or Latino!!White alone|\n",
    "|pct_black|Percent Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Not Hispanic or Latino!!Black or African American alone|\n",
    "|pct_asian|Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Not Hispanic or Latino!!Asian alone|\n",
    "|pct_male|Percent Estimate!!SEX AND AGE!!Total population!!Male|\n",
    "|pct_single_family_home|Percent Estimate!!UNITS IN STRUCTURE!!Total housing units!!1-unit detached|\n",
    "|med_home_value|Estimate!!VALUE!!Owner-occupied units!!Median (dollars)|\n",
    "|med_rooms_per_home|Estimate!!ROOMS!!Total housing units!!Median rooms|\n",
    "|pct_built_before_1940|Percent Estimate!!YEAR STRUCTURE BUILT!!Total housing units!!Built 1939 or earlier|\n",
    "|pct_renting|Percent Estimate!!HOUSING TENURE!!Occupied housing units!!Renter-occupied|\n",
    "|rental_vacancy_rate|Estimate!!HOUSING OCCUPANCY!!Total housing units!!Rental vacancy rate|\n",
    "|avg_renter_household_size|Estimate!!HOUSING TENURE!!Occupied housing units!!Average household size of renter-occupied unit|\n",
    "|med_gross_rent|Estimate!!GROSS RENT!!Occupied units paying rent!!Median (dollars)|\n",
    "|med_household_income|Estimate!!INCOME AND BENEFITS (IN 2017 INFLATION-ADJUSTED DOLLARS)!!Total households!!Median household income (dollars)|\n",
    "|mean_commute_time|Estimate!!COMMUTING TO WORK!!Workers 16 years and over!!Mean travel time to work (minutes)|\n",
    "|pct_commute_drive_alone|Percent Estimate!!COMMUTING TO WORK!!Workers 16 years and over!!Car truck or van drove alone|\n",
    "|pct_below_poverty|Percent Estimate!!PERCENTAGE OF FAMILIES AND PEOPLE WHOSE INCOME IN THE PAST 12 MONTHS IS BELOW THE POVERTY LEVEL!!All people|\n",
    "|pct_college_grad_student|Percent Estimate!!SCHOOL ENROLLMENT!!Population 3 years and over enrolled in school!!College or graduate school|\n",
    "|pct_same_residence_year_ago|Percent Estimate!!RESIDENCE 1 YEAR AGO!!Population 1 year and over!!Same house|\n",
    "|pct_bachelors_degree|Percent Estimate!!EDUCATIONAL ATTAINMENT!!Population 25 years and over!!Percent bachelor's degree or higher|\n",
    "|pct_english_only|Percent Estimate!!LANGUAGE SPOKEN AT HOME!!Population 5 years and over!!English only|\n",
    "|pct_foreign_born|Percent Estimate!!PLACE OF BIRTH!!Total population!!Foreign born|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3yA4dOqWRrA"
   },
   "source": [
    "## 3. Clean and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxtaYJ9pWRrA"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsYP2E5-WRrA"
   },
   "outputs": [],
   "source": [
    "# data types of the columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYQ8BIZMWRrA"
   },
   "outputs": [],
   "source": [
    "# access a single column like df['col_name']\n",
    "df[\"med_gross_rent\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEx_HyBgWRrA"
   },
   "outputs": [],
   "source": [
    "# pandas uses numpy's nan to represent null (missing) values\n",
    "print(np.nan)\n",
    "print(type(np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xvipM2xWRrB"
   },
   "outputs": [],
   "source": [
    "# convert rent from string -> float\n",
    "df[\"med_gross_rent\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKFC0WSjWRrB"
   },
   "source": [
    "Didn't work! We need to clean up the stray alphabetical characters to get a numerical value. You can do string operations on pandas Series to clean up their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qZV0mcoWRrB"
   },
   "outputs": [],
   "source": [
    "# do a string replace and assign back to that column, then change type to float\n",
    "df[\"med_gross_rent\"] = df[\"med_gross_rent\"].str.replace(\" (USD)\", \"\", regex=False)\n",
    "df[\"med_gross_rent\"] = df[\"med_gross_rent\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fywjt938WRrB"
   },
   "outputs": [],
   "source": [
    "# now clean up the income column then convert it from string -> float\n",
    "# do a string replace and assign back to that column\n",
    "df[\"med_household_income\"] = df[\"med_household_income\"].str.replace(\"$\", \"\", regex=False)\n",
    "df[\"med_household_income\"] = df[\"med_household_income\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YSrhBVEWRrB"
   },
   "outputs": [],
   "source": [
    "# convert rent from float -> int\n",
    "df[\"med_gross_rent\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsLC-CDNWRrB"
   },
   "source": [
    "You cannot store null values as type `int`, only as type `float`. You have three basic options:\n",
    "\n",
    "  1. Keep the column as float to retain the nulls - they are often important!\n",
    "  2. Drop all the rows that contain nulls if we need non-null data for our analysis\n",
    "  3. Fill in all the nulls with another value if we know a reliable default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HUeZkzoWRrB"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otkdAfe_WRrB"
   },
   "outputs": [],
   "source": [
    "# drop rows that contain nulls\n",
    "# this doesn't save result, because we didn't reassign! (in reality, want to keep the nulls here)\n",
    "df.dropna(subset=[\"med_gross_rent\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bL5OxRzCWRrB"
   },
   "outputs": [],
   "source": [
    "# fill in rows that contain nulls\n",
    "# this doesn't save result, because we didn't reassign! (in reality, want to keep the nulls here)\n",
    "df[\"med_gross_rent\"].fillna(value=0).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZTDGxj_WRrB"
   },
   "outputs": [],
   "source": [
    "# more string operations: slice state fips and county fips out of the tract fips string\n",
    "# assign them to new dataframe columns\n",
    "df[\"state\"] = df[\"GEOID10\"].str.slice(0, 2)\n",
    "df[\"county\"] = df[\"GEOID10\"].str.slice(2, 5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMX6iU4DWRrB"
   },
   "outputs": [],
   "source": [
    "# dict that maps state fips code -> state name\n",
    "fips = {\"04\": \"Arizona\", \"06\": \"California\", \"41\": \"Oregon\"}\n",
    "\n",
    "# replace fips code with state name with the replace() method\n",
    "df[\"state\"] = df[\"state\"].replace(fips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUJ0UfGeWRrB"
   },
   "outputs": [],
   "source": [
    "# you can rename columns with the rename() method\n",
    "# remember to reassign to save the result\n",
    "df = df.rename(columns={\"state\": \"state_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acl2QfXMWRrB"
   },
   "outputs": [],
   "source": [
    "# you can drop columns you don't need with the drop() method\n",
    "# remember to reassign to save the result\n",
    "df = df.drop(columns=[\"county\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4z8MYfxWRrB"
   },
   "outputs": [],
   "source": [
    "# inspect the cleaned-up dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yykJcHGLWRrB"
   },
   "outputs": [],
   "source": [
    "# save it to disk as a \"clean\" copy\n",
    "# note the filepath\n",
    "filepath = \"/content/drive/My Drive/Colab Notebooks/census_tracts_data_la-clean.csv\"\n",
    "df.to_csv(filepath, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRbWkKdHcaa8"
   },
   "outputs": [],
   "source": [
    "# and you can read it back in\n",
    "pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcER05eZWRrB"
   },
   "source": [
    "## 4. Selecting and slicing data from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtkQ5E6YWRrB"
   },
   "outputs": [],
   "source": [
    "# CHEAT SHEET OF COMMON TASKS\n",
    "# Operation                       Syntax           Result\n",
    "# ------------------------------------------------------------\n",
    "# Select column by name           df[col]          Series\n",
    "# Select columns by name          df[col_list]     DataFrame\n",
    "# Select row by label             df.loc[label]    Series\n",
    "# Select row by integer location  df.iloc[loc]     Series\n",
    "# Slice rows by label             df.loc[a:c]      DataFrame\n",
    "# Select rows by boolean vector   df[mask]         DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDJiVwsYWRrB"
   },
   "source": [
    "### 4a. Select DataFrame's column(s) by name\n",
    "\n",
    "We saw some of this a minute ago. Let's look in a bit more detail and break down what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46Ps0zvfWRrB"
   },
   "outputs": [],
   "source": [
    "# select a single column by column name\n",
    "# this is a pandas series\n",
    "df[\"total_pop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agd9lFcKWRrB"
   },
   "outputs": [],
   "source": [
    "# select multiple columns by a list of column names\n",
    "# this is a pandas dataframe that is a subset of the original\n",
    "df[[\"total_pop\", \"median_age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d6hzRbiWRrC"
   },
   "outputs": [],
   "source": [
    "# create a new column by assigning df['new_col'] to some set of values\n",
    "# you can do math operations on any numeric columns\n",
    "df[\"monthly_income\"] = df[\"med_household_income\"] / 12\n",
    "df[\"rent_burden\"] = df[\"med_gross_rent\"] / df[\"monthly_income\"]\n",
    "\n",
    "# inspect the results\n",
    "df[[\"med_household_income\", \"monthly_income\", \"med_gross_rent\", \"rent_burden\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1Cdn9yrWRrC"
   },
   "source": [
    "### 4b. Select row(s) by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbDB9uZ9WRrC"
   },
   "outputs": [],
   "source": [
    "# use .loc to select by row label\n",
    "# returns the row as a series whose index is the dataframe column names\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhaFEIzoWRrC"
   },
   "outputs": [],
   "source": [
    "# use .loc to select single value by row label, column name\n",
    "df.loc[0, \"pct_below_poverty\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4kHFOobWRrC"
   },
   "outputs": [],
   "source": [
    "# slice of rows from label 5 to label 7, inclusive\n",
    "# this returns a pandas dataframe\n",
    "df.loc[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pKs6PC4WRrC"
   },
   "outputs": [],
   "source": [
    "# slice of rows from label 1 to label 3, inclusive\n",
    "# slice of columns from pct_hispanic to pct_asian, inclusive\n",
    "df.loc[1:3, \"pct_hispanic\":\"pct_asian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpPTqs8sWRrC"
   },
   "outputs": [],
   "source": [
    "# subset of rows from with labels in list\n",
    "# subset of columns with names in list\n",
    "df.loc[[1, 3], [\"pct_hispanic\", \"pct_asian\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYJXGJtPWRrC"
   },
   "outputs": [],
   "source": [
    "# you can use a column of unique identifiers as the index\n",
    "# fips codes uniquely identify each row (but verify!)\n",
    "df = df.set_index(\"GEOID10\")\n",
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0z5uCWCZWRrC"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG8ttwyfWRrC"
   },
   "outputs": [],
   "source": [
    "# .loc works by label, not by position in the dataframe\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9z3Fc_aDWRrC"
   },
   "outputs": [],
   "source": [
    "# the index now contains fips codes, so you have to use .loc accordingly to select by row label\n",
    "df.loc[\"06037137201\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJkyVo_ZWRrC"
   },
   "source": [
    "### 4c. Select by (integer) position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kewRgi2gWRrD"
   },
   "outputs": [],
   "source": [
    "# get the row in the zero-th position in the dataframe\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhTLkK7sWRrD"
   },
   "outputs": [],
   "source": [
    "# you can slice as well\n",
    "# note, while .loc[] is inclusive, .iloc[] is not\n",
    "# get the rows from position 0 up to but not including position 3 (ie, rows 0, 1, and 2)\n",
    "df.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVeo8kHaWRrD"
   },
   "outputs": [],
   "source": [
    "# get the value from the row in position 3 and the column in position 2 (zero-indexed)\n",
    "df.iloc[3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2MBT01zWRrD"
   },
   "source": [
    "### 4d. Select/filter by value\n",
    "\n",
    "You can subset or filter a dataframe for based on the values in its rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNBqLAIpWRrD"
   },
   "outputs": [],
   "source": [
    "# filter the dataframe by rows with 30%+ rent burden\n",
    "df[df[\"rent_burden\"] > 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYdyauntWRrD"
   },
   "outputs": [],
   "source": [
    "# what exactly did that do? let's break it out.\n",
    "df[\"rent_burden\"] > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bW4zGUBWRrD"
   },
   "outputs": [],
   "source": [
    "# essentially a true/false mask that filters by value\n",
    "mask = df[\"rent_burden\"] > 0.3\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJP51-eVWRrD"
   },
   "outputs": [],
   "source": [
    "# you can chain multiple conditions together\n",
    "# pandas logical operators are: | for or, & for and, ~ for not\n",
    "# these must be grouped by using parentheses due to order of operations\n",
    "# question: which tracts are both rent-burdened and majority-Black?\n",
    "mask = (df[\"rent_burden\"] > 0.3) & (df[\"pct_black\"] > 50)\n",
    "df[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4Ib48SzWRrD"
   },
   "outputs": [],
   "source": [
    "# which tracts are both rent-burdened and either majority-Black or majority-Hispanic?\n",
    "mask1 = df[\"rent_burden\"] > 0.3\n",
    "mask2 = df[\"pct_black\"] > 50\n",
    "mask3 = df[\"pct_hispanic\"] > 50\n",
    "mask = mask1 & (mask2 | mask3)\n",
    "df[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xZdfIS8WRrD"
   },
   "outputs": [],
   "source": [
    "# see the mask\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtICZ6hOWRrD"
   },
   "outputs": [],
   "source": [
    "# ~ means not... it essentially flips trues to falses and vice-versa\n",
    "~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jpy5cZFWRrD"
   },
   "outputs": [],
   "source": [
    "# which rows are in a state that begins with \"Cal\"?\n",
    "# all of them... because we're looking only at LA county\n",
    "mask = df[\"state_name\"].str.startswith(\"Cal\")\n",
    "df[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPiBqLRKWRrD"
   },
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# create a new subset dataframe containing all the rows with median home values\n",
    "# above $800,000 and percent-White above 60%... how many rows did you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUhMU19ZWRrD"
   },
   "source": [
    "## 5. Descriptive stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YhQEn-JWRrD"
   },
   "outputs": [],
   "source": [
    "# what share of majority-White tracts are rent burdened?\n",
    "mask1 = df[\"pct_white\"] > 50\n",
    "mask2 = mask1 & (df[\"rent_burden\"] > 0.3)\n",
    "len(df[mask2]) / len(df[mask1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nmob7tI7WRrF"
   },
   "outputs": [],
   "source": [
    "# what share of majority-Hispanic tracts are rent burdened?\n",
    "mask1 = df[\"pct_hispanic\"] > 50\n",
    "mask2 = mask1 & (df[\"rent_burden\"] > 0.3)\n",
    "len(df[mask2]) / len(df[mask1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEYh-UAdWRrF"
   },
   "outputs": [],
   "source": [
    "# you can sort the dataframe by values in some column\n",
    "df.sort_values(\"pct_below_poverty\", ascending=False).dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHY833TFWRrF"
   },
   "outputs": [],
   "source": [
    "# use the describe() method to pull basic descriptive stats for some column\n",
    "df[\"med_household_income\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7LLMwncWRrF"
   },
   "source": [
    "#### Or if you need the value of a single stat, call it directly\n",
    "\n",
    "Key measures of central tendency: mean and median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhFiaRnfWRrF"
   },
   "outputs": [],
   "source": [
    "# the mean, or \"average\" value\n",
    "df[\"med_household_income\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyk8eZLyWRrF"
   },
   "outputs": [],
   "source": [
    "# the median, or \"typical\" (ie, 50th percentile) value\n",
    "df[\"med_household_income\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LFz56HcWRrF"
   },
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# create a new subset dataframe containing rows with median household income above the (tract)\n",
    "# average in LA county... what is the median median home value across this subset of tracts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wkp_2Xf0WRrF"
   },
   "source": [
    "Key measures of dispersion or variability: range, IQR, variance, standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UYRIjvDWRrF"
   },
   "outputs": [],
   "source": [
    "df[\"med_household_income\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "On8o5fFTWRrF"
   },
   "outputs": [],
   "source": [
    "# which tract has the lowest median household income?\n",
    "df[\"med_household_income\"].idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtiPA4ZkWRrF"
   },
   "outputs": [],
   "source": [
    "df[\"med_household_income\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NnMe8gwWRrF"
   },
   "outputs": [],
   "source": [
    "# what is the 90th-percentile value?\n",
    "df[\"med_household_income\"].quantile(0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHLLdij4WRrF"
   },
   "outputs": [],
   "source": [
    "# calculate the distribution's range\n",
    "df[\"med_household_income\"].max() - df[\"med_household_income\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnkK3FxCWRrF"
   },
   "outputs": [],
   "source": [
    "# calculate its IQR\n",
    "df[\"med_household_income\"].quantile(0.75) - df[\"med_household_income\"].quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRLm-m7QWRrF"
   },
   "outputs": [],
   "source": [
    "# calculate its variance... rarely used in practice\n",
    "df[\"med_household_income\"].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7LHvbDTWRrF"
   },
   "outputs": [],
   "source": [
    "# calculate its standard deviation\n",
    "# this is the sqrt of the variance... putting it into same units as the variable itself\n",
    "df[\"med_household_income\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rl1sTvQDWRrG"
   },
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# what's the average (mean) median home value across majority-White tracts?\n",
    "# And across majority-Black tracts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiy_7_EYWRrG"
   },
   "source": [
    "## 6. Merge and concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGyIYIS7WRrG"
   },
   "source": [
    "### 6a. Merging DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OV2-AlR6WRrG"
   },
   "outputs": [],
   "source": [
    "# create a subset dataframe with only race/ethnicity variables\n",
    "race_cols = [\"pct_asian\", \"pct_black\", \"pct_hispanic\", \"pct_white\"]\n",
    "df_race = df[race_cols]\n",
    "df_race.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1jmg4X9WRrG"
   },
   "outputs": [],
   "source": [
    "# create a subset dataframe with only economic variables\n",
    "econ_cols = [\"med_home_value\", \"med_household_income\"]\n",
    "df_econ = df[econ_cols].sort_values(\"med_household_income\")\n",
    "df_econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7j8_Iz6WRrG"
   },
   "outputs": [],
   "source": [
    "# merge them together, aligning rows based on their labels in the index\n",
    "df_merged = pd.merge(left=df_econ, right=df_race, how=\"inner\", left_index=True, right_index=True)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4eKiYiYWRrG"
   },
   "outputs": [],
   "source": [
    "# reset df_econ's index\n",
    "df_econ = df_econ.reset_index()\n",
    "df_econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NM6GBwp9WRrG"
   },
   "outputs": [],
   "source": [
    "# merge them together, aligning rows based on their labels in the index\n",
    "# doesn't work! their indexes do not share any labels to match/align the rows\n",
    "df_merged = pd.merge(left=df_econ, right=df_race, how=\"inner\", left_index=True, right_index=True)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3eiEkwwWRrG"
   },
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# change the \"how\" argument: what happens if you try an \"outer\" join?\n",
    "# or a \"left\" join? or a \"right\" join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsLzoWqQWRrG"
   },
   "outputs": [],
   "source": [
    "# instead merge where df_race index matches df_econ GEOID10 column\n",
    "df_merged = pd.merge(left=df_econ, right=df_race, how=\"inner\", left_on=\"GEOID10\", right_index=True)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRqw71HoWRrG"
   },
   "source": [
    "### 6b. Concatenating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2pe5Vh-WRrG"
   },
   "outputs": [],
   "source": [
    "# load the orange county tracts data\n",
    "filepath = \"https://raw.githubusercontent.com/gboeing/ppd534/main/data/census_tracts_data_oc.csv\"\n",
    "oc = pd.read_csv(filepath, dtype={\"GEOID10\": str})\n",
    "oc = oc.set_index(\"GEOID10\")\n",
    "oc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PY2OprtIWRrG"
   },
   "outputs": [],
   "source": [
    "oc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_Max2FYWRrG"
   },
   "outputs": [],
   "source": [
    "# merging joins data together aligned by the index, but concatenating just smushes\n",
    "# it together along some axis\n",
    "df_all = pd.concat([df, oc], sort=False)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S7x3EcBWRrG"
   },
   "source": [
    "## 7. Grouping and summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9yX6X2UWRrG"
   },
   "outputs": [],
   "source": [
    "# extract county fips from index then replace with friendly name\n",
    "df_all[\"county\"] = df_all.index.str.slice(2, 5)\n",
    "df_all[\"county\"] = df_all[\"county\"].replace({\"037\": \"LA\", \"059\": \"OC\"})\n",
    "df_all[\"county\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXlNg_J_WRrG"
   },
   "outputs": [],
   "source": [
    "# group the rows by county\n",
    "counties = df_all.groupby(\"county\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ztmUPDNWRrG"
   },
   "outputs": [],
   "source": [
    "# what is the median pct_white across the tracts in each county?\n",
    "counties[\"pct_white\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12btiWpQWRrG"
   },
   "outputs": [],
   "source": [
    "# look at several columns' medians by county\n",
    "counties[[\"pct_bachelors_degree\", \"pct_foreign_born\", \"pct_commute_drive_alone\"]].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEkHq9XJWRrG"
   },
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# group the tracts by county and find the highest/lowest tract percentages that speak English-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnxEZ1TMWRrG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
